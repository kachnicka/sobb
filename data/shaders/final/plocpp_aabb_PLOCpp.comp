#version 460

#extension GL_EXT_buffer_reference2: require
#extension GL_EXT_scalar_block_layout: require
#extension GL_EXT_shader_atomic_int64: require
#extension GL_GOOGLE_include_directive : enable

#extension GL_KHR_memory_scope_semantics : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_arithmetic : require

#define INCLUDE_FROM_SHADER
#include "shared/types.glsl"
#include "shared/compute.glsl"
#include "shared/bv_aabb.glsl"
#include "shared/data_bvh.h"
#include "shared/data_plocpp.h"

layout(local_size_x_id = 0) in;
layout(constant_id = 1) const u32 sc_SubgroupSize = 32;
layout(constant_id = 2) const u32 sc_PlocRadius = 16;

const u32 sc_SubgroupCount = gl_WorkGroupSize.x / sc_SubgroupSize;
const u32 sc_ChunkSize = gl_WorkGroupSize.x - 4 * sc_PlocRadius;
const u32 sc_2PlocRadius = 2 * sc_PlocRadius;

layout(push_constant, scalar) uniform uPushConstant {
    PC_PlocppIterationIndirect data;
} pc;

#define SCHEDULER_DATA_ADDRESS pc.data.runtimeDataAddress
#include "shared/task_scheduler.glsl"

shared AABB cache[gl_WorkGroupSize.x];
shared u64 nn[gl_WorkGroupSize.x];

shared u32 sharedPrefix;
shared u32 sharedAggregate;
shared u32 validIdsInSubgroup[sc_SubgroupCount];

shared u32 sharedClusterCount;
shared u32 sharedTaskCount;

layout(buffer_reference, scalar) buffer RuntimeData {
    Scheduler sched;
    PLOCData ploc;
};

void merge(in u32 chunkId, in u32 clusterCount) {
    BVH2_AABB bvh = BVH2_AABB(pc.data.bvhAddress);

    u32_buf nodeId0 = u32_buf(pc.data.nodeId0Address);
    u32_buf nodeId1 = u32_buf(pc.data.nodeId1Address);
    RuntimeData data = RuntimeData(pc.data.runtimeDataAddress);

    if (gl_LocalInvocationID.x == 0)
        sharedAggregate = 0;
    barrier();

    const u32 myChunkIdOffset = chunkId * sc_ChunkSize;
    const u32 memoryIdLocal = gl_LocalInvocationID.x;
    const u32 memoryIdGlobal = memoryIdLocal + myChunkIdOffset - sc_2PlocRadius;
    const u32 myClusterCount = min(sc_ChunkSize, clusterCount - myChunkIdOffset);
    u32 myNodeId = INVALID_ID;

    // load clusters to shared memory, first sc_2PlocRadius invocations
    // will overflow the u32 and correctly load the dummy bounding volume
    if (memoryIdLocal < (myClusterCount + 4 * sc_PlocRadius)) {
        nn[memoryIdLocal] = INVALID_ID;
        if (memoryIdGlobal >= clusterCount)
            cache[memoryIdLocal] = dummyAABB();
        else {
            myNodeId = nodeId0.val[memoryIdGlobal];
            cache[memoryIdLocal] = bvh.node[myNodeId].bv;
        }
    }
    barrier();

    // find nearest neighbours
    if (memoryIdLocal < (myClusterCount + 3 * sc_PlocRadius)) {
        u64 minValue = INVALID_ID;
        AABB myBv = cache[memoryIdLocal];
        for (u32 cnt = 0; cnt < sc_PlocRadius; cnt++) {
            u32 cacheId = memoryIdLocal + cnt + 1;
            AABB neighborBv = cache[cacheId];
            bvFit(neighborBv, myBv);

            u64 encoded = (u64(floatBitsToUint(bvArea(neighborBv))) << 32);
            minValue = min(minValue, encoded | (cacheId));
            atomicMin(nn[cacheId], encoded | (memoryIdLocal));
        }
        atomicMin(nn[memoryIdLocal], minValue);
    }
    barrier();

    // merge corresponding NNs
    if ((memoryIdLocal >= sc_2PlocRadius) && (memoryIdLocal < (myClusterCount + sc_2PlocRadius))) {
        u32 myNeighbour = u32(nn[memoryIdLocal]);
        u32 hisNeighbour = u32(nn[myNeighbour]);
        bool isValidId = true;

        if (memoryIdLocal == hisNeighbour) {
            if (memoryIdLocal < myNeighbour) {
                const uvec4 ballot = subgroupBallot(true);
                u32 mergedId;
                if (subgroupElect())
                    mergedId = atomicAdd(data.ploc.bvOffset, subgroupBallotBitCount(ballot));
                mergedId = subgroupBroadcastFirst(mergedId) + subgroupBallotExclusiveBitCount(ballot);

                const u32 rightNodeId = nodeId0.val[memoryIdGlobal + (myNeighbour - hisNeighbour)];

                AABB bvC0 = cache[memoryIdLocal];
                bvFit(bvC0, cache[myNeighbour]);

                i32 sizeC0 = bvh.node[myNodeId].size;
                i32 sizeC1 = bvh.node[rightNodeId].size;
                // i32 size = bvh.node[myNodeId].size + bvh.node[rightNodeId].size;
                bvh.node[myNodeId].parent = i32(mergedId);
                bvh.node[rightNodeId].parent = i32(mergedId);

                bvh.node[mergedId].bv = bvC0;
                bvh.node[mergedId].size = abs(sizeC0) + abs(sizeC1);
                bvh.node[mergedId].parent = INVALID_ID;
                // bvh.node[mergedId].c0 = i32(myNodeId);
                // bvh.node[mergedId].c1 = i32(rightNodeId);
                bvh.node[mergedId].c0 = sizeC0 < 0 ? ~i32(myNodeId) : i32(myNodeId);
                bvh.node[mergedId].c1 = sizeC1 < 0 ? ~i32(rightNodeId) : i32(rightNodeId);

                nodeId1.val[memoryIdGlobal] = mergedId;
                nodeId1.val[memoryIdGlobal + (myNeighbour - hisNeighbour)] = INVALID_ID;
            }
            else
                isValidId = false;
        }
        else
            nodeId1.val[memoryIdGlobal] = myNodeId;

        const uvec4 ballot = subgroupBallot(isValidId);
        if (subgroupElect())
            atomicAdd(sharedAggregate, subgroupBallotBitCount(ballot));
    }
    barrier();

    if (memoryIdLocal == sc_2PlocRadius) {
        DLWork_buf dlWork = DLWork_buf(pc.data.dlWorkBufAddress);
        dlWork.val[chunkId].aggregate = sharedAggregate;
        dlWork.val[chunkId].prefix = (chunkId == 0) ? sharedAggregate : INVALID_ID;
    }
}

void compact(in u32 chunkId, in u32 clusterCount) {
    u32_buf nodeId0 = u32_buf(pc.data.nodeId0Address);
    u32_buf nodeId1 = u32_buf(pc.data.nodeId1Address);
    DLWork_buf dlWork = DLWork_buf(pc.data.dlWorkBufAddress);

    u32 myClusterCount = min(sc_ChunkSize, clusterCount - chunkId * sc_ChunkSize);
    u32 memoryIdGlobal = chunkId * sc_ChunkSize - sc_2PlocRadius + gl_LocalInvocationID.x;
    u32 memoryIdLocal = gl_LocalInvocationID.x;

    if (gl_LocalInvocationID.x == 0)
        sharedPrefix = 0;
    if (gl_LocalInvocationID.x < sc_SubgroupSize)
        validIdsInSubgroup[gl_LocalInvocationID.x] = 0;
    barrier();

    if ((memoryIdLocal == sc_2PlocRadius) && (chunkId > 0)) {
        u32 lookbackPartitionId = chunkId - 1;
        while (true) {
            if (atomicLoad(dlWork.val[lookbackPartitionId].prefix, gl_ScopeQueueFamily, gl_StorageSemanticsBuffer,
                    gl_SemanticsAcquire | gl_SemanticsMakeVisible) != INVALID_ID) {
                atomicAdd(sharedPrefix, dlWork.val[lookbackPartitionId].prefix);
                break;
            }
            else {
                atomicAdd(sharedPrefix, dlWork.val[lookbackPartitionId].aggregate);
                lookbackPartitionId--;
            }
        }
        atomicStore(dlWork.val[chunkId].prefix, dlWork.val[chunkId].aggregate + sharedPrefix,
            gl_ScopeQueueFamily, gl_StorageSemanticsBuffer, gl_SemanticsRelease | gl_SemanticsMakeAvailable);
    }
    barrier();

    // finalize the prefix sum computation and perform the ID compaction
    u32 bvId;
    uvec4 ballot;
    if ((memoryIdLocal >= sc_2PlocRadius) && (memoryIdLocal < (myClusterCount + sc_2PlocRadius))) {
        bvId = nodeId1.val[memoryIdGlobal];
        ballot = subgroupBallot(bvId != INVALID_ID);
        if (subgroupElect())
            validIdsInSubgroup[gl_SubgroupID] = subgroupBallotBitCount(ballot);
    }
    barrier();
    if (gl_LocalInvocationID.x < sc_SubgroupSize)
        validIdsInSubgroup[gl_LocalInvocationID.x] = sharedPrefix + subgroupExclusiveAdd(validIdsInSubgroup[gl_LocalInvocationID.x]);
    barrier();
    if ((memoryIdLocal >= sc_2PlocRadius) && (memoryIdLocal < (myClusterCount + sc_2PlocRadius))) {
        if (bvId != INVALID_ID) {
            u32 compactedId = validIdsInSubgroup[gl_SubgroupID] + subgroupBallotExclusiveBitCount(ballot);
            nodeId0.val[compactedId] = bvId;
        }
    }
}

#define PLOC_PHASE_DONE 0
#define PLOC_PHASE_MERGE 3
#define PLOC_PHASE_COMPACT 4

// each workgroup handles the PLOCpp chunk of the clusters
// chunk represents also the decoupled lookback partition for prefix scan
void main() {
    DLWork_buf dlWork = DLWork_buf(pc.data.dlWorkBufAddress);
    RuntimeData data = RuntimeData(pc.data.runtimeDataAddress);

    if (gl_GlobalInvocationID.x == 0) {
        u32_buf clusterCount = u32_buf(pc.data.idbAddress);
        data.ploc.bvOffset = clusterCount.val[0];
        data.ploc.iterationClusterCount = data.ploc.bvOffset;

        u32 newTaskCount = divCeil(data.ploc.bvOffset, sc_ChunkSize);
        data.ploc.iterationTaskCount = newTaskCount;
        allocTasks(newTaskCount, PLOC_PHASE_MERGE);
    }

    while (true) {
        Task task = beginTask(gl_LocalInvocationID.x);
        if (gl_LocalInvocationID.x == 0) {
            sharedClusterCount = data.ploc.iterationClusterCount;
            sharedTaskCount = data.ploc.iterationTaskCount;
        }
        barrier();

        switch (task.phase) {
            case PLOC_PHASE_MERGE:
            merge(task.id, sharedClusterCount);

            if (endTask(gl_LocalInvocationID.x)) {
                allocTasks(sharedTaskCount, PLOC_PHASE_COMPACT);
                data.ploc.iterationCounter++;
            }
            break;
            case PLOC_PHASE_COMPACT:
            compact(task.id, sharedClusterCount);

            if (endTask(gl_LocalInvocationID.x)) {
                u32 newClusterCount = dlWork.val[sharedTaskCount - 1].prefix;
                u32 newTaskCount = divCeil(newClusterCount, sc_ChunkSize);

                if (newClusterCount > 1) {
                    data.ploc.iterationClusterCount = newClusterCount;
                    data.ploc.iterationTaskCount = newTaskCount;
                    allocTasks(newTaskCount, PLOC_PHASE_MERGE);
                }
                else
                    allocTasks(gl_NumWorkGroups.x, PLOC_PHASE_DONE);
            }
            break;
            case PLOC_PHASE_DONE:
            return;
        }
    }
}
